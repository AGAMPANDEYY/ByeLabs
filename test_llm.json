{
  "model": "/root/.cache/llama.cpp/unsloth_Qwen3-4B-Instruct-2507-GGUF_Qwen3-4B-Instruct-2507-Q4_K_M.gguf",
  "messages": [
    {
      "role": "user", 
      "content": "Hello, can you respond with just 'LLM is working'?"
    }
  ],
  "max_tokens": 10,
  "temperature": 0.1
}